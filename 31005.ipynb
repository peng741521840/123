{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "31005.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMs0Di570rX9u0GI4FwIWCC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peng741521840/123/blob/main/31005.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbheEJEDpOZ2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math, sys, os"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts8-EpyUptak"
      },
      "source": [
        "def dataProcess(source_fpath, target_fpath):\n",
        "    with open(source_fpath) as source_f:\n",
        "        sample_list = []\n",
        "        for line in source_f:\n",
        "            content = line.strip().split(\",\")\n",
        "            sample_list.append(np.array(content))\n",
        "        csvdf = pd.DataFrame(sample_list)\n",
        "        csvdf.columns = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\", \"Class\"]\n",
        "        csvdf.to_csv(target_fpath, index=0)"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ca1yjNGG7Ze",
        "outputId": "f2373935-fd28-4ff0-a4c1-4c2b56f04f5f"
      },
      "source": [
        "dataset = pd.read_csv(\"iris.csv\")\n",
        "print(dataset)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     SepalLength  SepalWidth  PetalLength  PetalWidth           Class\n",
            "0            5.1         3.5          1.4         0.2     Iris-setosa\n",
            "1            4.9         3.0          1.4         0.2     Iris-setosa\n",
            "2            4.7         3.2          1.3         0.2     Iris-setosa\n",
            "3            4.6         3.1          1.5         0.2     Iris-setosa\n",
            "4            5.0         3.6          1.4         0.2     Iris-setosa\n",
            "..           ...         ...          ...         ...             ...\n",
            "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
            "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
            "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
            "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
            "150          NaN         NaN          NaN         NaN             NaN\n",
            "\n",
            "[151 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6lFO2JopTfB"
      },
      "source": [
        "def informationEntropy(dataset):\n",
        "    entropysum = 0\n",
        "    category_list = list(dataset[\"Class\"])\n",
        "    for category in set(dataset[\"Class\"]):\n",
        "        pk = category_list.count(category) / len(dataset)\n",
        "        entropysum += pk * math.log(pk, 2)\n",
        "    return (-1) * entropysum"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f5F0JYxT5j0"
      },
      "source": [
        "def informationDiscreteGain(dataset, attribute):\n",
        "    entropy = informationEntropy(dataset)\n",
        "    entropysum = 0\n",
        "    attribute_value_list = list(dataset[attribute])\n",
        "    for attribute_value in set(dataset[attribute]):\n",
        "        weight = attribute_value_list.count(attribute_value) / len(dataset)\n",
        "        entropysum += weight * informationEntropy(dataset[dataset[attribute] == attribute_value])\n",
        "    return entropy - entropysum"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LreGQMotpbZS"
      },
      "source": [
        "def maxNumOutcome(dataset):\n",
        "    category_list = list(dataset[\"Class\"])\n",
        "    category_dict = {}\n",
        "    for category in set(dataset[\"Class\"]):\n",
        "        category_dict[category] = category_list.count(category)\n",
        "    category_sorted = sorted(category_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "    return category_sorted[0][0]"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k80rZtCtpgZO"
      },
      "source": [
        "def treeNodeGenerate(dataset, attribute_list):\n",
        "    if len(set(dataset[\"Class\"])) == 1:\n",
        "        node = list(set(dataset[\"Class\"]))[0]\n",
        "    elif len(attribute_list) == 0 or sum([len(set(dataset[attribute])) - 1 for attribute in attribute_list]) == 0:\n",
        "        node = maxNumOutcome(dataset)\n",
        "    else:\n",
        "        attribute_gain_dict = {}\n",
        "        for attribute in attribute_list:\n",
        "            threshold, attribute_gain = informationContinuousGain(dataset, attribute)\n",
        "            attribute_gain_dict[attribute] = threshold, attribute_gain\n",
        "        attribute_gain_sorted = sorted(attribute_gain_dict.items(), key=lambda item: item[1][1], reverse=True)\n",
        "        maxgain_attribute = attribute_gain_sorted[0][0]\n",
        "        maxgain_threshold = attribute_gain_sorted[0][1][0]\n",
        "\n",
        "        son_node_attribute_list = attribute_list.copy()\n",
        "        son_node_attribute_list.remove(maxgain_attribute)\n",
        "\n",
        "        left_node_dataset = dataset[dataset[maxgain_attribute] <= maxgain_threshold]\n",
        "        if len(left_node_dataset) == 0:\n",
        "            leftnode = maxNumOutcome(dataset)\n",
        "        else:\n",
        "            leftnode = treeNodeGenerate(left_node_dataset, son_node_attribute_list)\n",
        "        \n",
        "        right_node_dataset = dataset[dataset[maxgain_attribute] > maxgain_threshold]\n",
        "        if len(right_node_dataset) == 0:\n",
        "            rightnode = maxNumOutcome(dataset)\n",
        "        else:\n",
        "            rightnode = treeNodeGenerate(right_node_dataset, son_node_attribute_list)\n",
        "        \n",
        "        if leftnode == rightnode:\n",
        "            node = leftnode\n",
        "        else:\n",
        "            node = {}\n",
        "            node[(maxgain_attribute, maxgain_threshold)] = {\"<=\":leftnode, \">\":rightnode}\n",
        "\n",
        "    return node"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RrqEs6Ppi1x"
      },
      "source": [
        "def predictOne(tree_train_model, testdata):\n",
        "    if type(tree_train_model) == str:\n",
        "        predict_value = tree_train_model\n",
        "    elif type(tree_train_model) == dict:\n",
        "        key = list(tree_train_model)[0]\n",
        "        if testdata[key[0]] <= key[1]:\n",
        "            son_tree_train_model = tree_train_model[key][\"<=\"]\n",
        "        else:\n",
        "            son_tree_train_model = tree_train_model[key][\">\"]\n",
        "        predict_value = predictOne(son_tree_train_model, testdata)\n",
        "    return predict_value"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlMpZ8r2pkC8"
      },
      "source": [
        "def predict(tree_train_model, testdataset):\n",
        "    predict_list = []\n",
        "    for i in range(len(testdataset)):\n",
        "        predict_value = predictOne(tree_train_model, testdataset.loc[i])\n",
        "        predict_list.append((testdataset.loc[i][\"Class\"], predict_value))\n",
        "    return predict_list"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cadrgq_pplJv"
      },
      "source": [
        "def predictAccuracy(predict_list):\n",
        "    predict_true_num = 0\n",
        "    for bigram in predict_list:\n",
        "        if bigram[0] == bigram[1]:\n",
        "            predict_true_num += 1\n",
        "    accuracy = predict_true_num / len(predict_list)\n",
        "    return accuracy"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDiw_dfCpm4z"
      },
      "source": [
        "def subdatasetPartitioning(dataset):\n",
        "    index = [i for i in range(len(dataset))]\n",
        "    np.random.seed(2)\n",
        "    np.random.shuffle(index)\n",
        "\n",
        "    traindatasetlen = int(len(dataset) * 0.8)\n",
        "    traindataset = dataset.loc[index[:traindatasetlen]]\n",
        "    testdataset = dataset.loc[index[traindatasetlen:]]\n",
        "\n",
        "    return traindataset, testdataset"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo9C45OUpn8p"
      },
      "source": [
        "def datasetPartitioning(dataset):\n",
        "\n",
        "    traindataset_list = []\n",
        "    testdataset_list = []\n",
        "    for i in range(3):\n",
        "        subdataset = dataset.loc[i * 50 : (i + 1) * 50 - 1]\n",
        "        subdataset = subdataset.reset_index()\n",
        "        subtraindataset, subtestdataset = subdatasetPartitioning(subdataset)\n",
        "        traindataset_list.append(subtraindataset)\n",
        "        testdataset_list.append(subtestdataset)\n",
        "\n",
        "    traindataset = pd.concat(traindataset_list, ignore_index=True)\n",
        "    testdataset = pd.concat(testdataset_list, ignore_index=True)\n",
        "\n",
        "    return traindataset, testdataset"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZXFq2azppka",
        "outputId": "121cc50e-d8d9-41ca-8740-eaeab546393d"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    source_fpath = \"iris.data\"\n",
        "    target_fpath = source_fpath.replace(\"data\", \"csv\")\n",
        "    dataProcess(source_fpath, target_fpath)\n",
        "\n",
        "    dataset = pd.read_csv(\"iris.csv\")\n",
        "\n",
        "    traindataset, testdataset =  datasetPartitioning(dataset)\n",
        "\n",
        "    attribute_list = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]\n",
        "    tree_train_model = treeNodeGenerate(traindataset, attribute_list)\n",
        "    print(\"The Dict of Trained Model:\")\n",
        "    print(tree_train_model, \"\\n\")\n",
        "\n",
        "    predict_list = predict(tree_train_model, testdataset)\n",
        "    print(\"The List of Predicting Outcomes (Actual Label, Predicted Value) :\")\n",
        "    print(predict_list, \"\\n\")\n",
        "\n",
        "    print(\"The Accuracy of Model Prediction: \", predictAccuracy(predict_list))"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Dict of Trained Model:\n",
            "{('PetalLength', 2.45): {'<=': 'Iris-setosa', '>': {('PetalWidth', 1.75): {'<=': {('SepalLength', 4.95): {'<=': 'Iris-virginica', '>': 'Iris-versicolor'}}, '>': {('SepalLength', 5.95): {'<=': {('SepalWidth', 3.0): {'<=': 'Iris-virginica', '>': 'Iris-versicolor'}}, '>': 'Iris-virginica'}}}}}} \n",
            "\n",
            "The List of Predicting Outcomes (Actual Label, Predicted Value) :\n",
            "[('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-setosa', 'Iris-setosa'), ('Iris-versicolor', 'Iris-virginica'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-versicolor', 'Iris-versicolor'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica'), ('Iris-virginica', 'Iris-virginica')] \n",
            "\n",
            "The Accuracy of Model Prediction:  0.9666666666666667\n"
          ]
        }
      ]
    }
  ]
}